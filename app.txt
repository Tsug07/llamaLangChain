# app.py
from langchain_ollama import OllamaLLM
import streamlit as st

# Inicializando o modelo
llm = OllamaLLM(model="llama3.2")  # Substitua pelo modelo que você está usando

# Título do aplicativo
st.title("Chat com o Modelo Ollama")

# Caixa de entrada para o usuário
user_input = st.text_input("Digite sua pergunta:")

# Botão para enviar a pergunta
if st.button("Enviar"):
    if user_input:
        # Invocando o modelo
        try:
            response = llm.invoke(user_input)
            st.write("Resposta do modelo:", response)
        except Exception as e:
            st.error(f"Erro ao invocar o modelo: {e}")
    else:
        st.warning("Por favor, digite uma pergunta.")
